{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVkWRBKjW9cz"
   },
   "outputs": [],
   "source": [
    "############# Install the dependencies ##################\n",
    "# !pip install datasets transformers evaluate sacrebleu\n",
    "\n",
    "####   if code fails below while creating the training arguments , run the following and restart the kernel   #######\n",
    "# !pip install --upgrade accelerate\n",
    "\n",
    "\n",
    "# !pip uninstall -y transformers accelerate\n",
    "# !pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the dataset from repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "84052cfce5474a029f3cb674bcee542b",
      "3a33b5c78f624af7a1cfe0582f741181",
      "1195736dd204458fa55bebf3122865fb",
      "0df3fe5fed0c468aa52c7b84e835ad87",
      "84d444ff75684a299502f2c42ba01e3b",
      "fca24beda7104313991ef9ab9f3789c6",
      "de5eccf0d41042ac85b1cc22155ebf47",
      "f79ce623076f452b8b49b7e87ae78195",
      "76167c9dba6f462e82a5ea04aa706839",
      "44be907aa8c040218eb745e7dabfd056",
      "406f0b6143704350a9d2cbeb830d91bf"
     ]
    },
    "id": "bbVcCsCOYFMw",
    "outputId": "f9fe49c9-ffa6-4272-d7fb-a5cf971520a0"
   },
   "outputs": [],
   "source": [
    "# ## Import the dataset on which we want to fine-tune the model\n",
    "# # I have used ai4bharat/samanantar, don't forget to look for the subsets of the dataset and load it accordingly\n",
    "# from datasets import load_dataset\n",
    "# odia=load_dataset(\"ai4bharat/samanantar\",\"or\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XriknqLO9PGW",
    "outputId": "cfe8c0e9-115d-4b67-c032-4dd47c395725"
   },
   "outputs": [],
   "source": [
    "# print(odia)\n",
    "# Our dataset has three features , src has english text while tgt has odia tranlations of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "db6thz0H-yCo"
   },
   "outputs": [],
   "source": [
    "##Import Tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLVGjuE-LddV"
   },
   "outputs": [],
   "source": [
    "# odia[\"train\"]['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0gvxFz2j-oU"
   },
   "outputs": [],
   "source": [
    "# ## Tokenization function\n",
    "def preprocess(odia):\n",
    "    # putting all the English sentences into the imput list with the prefix.\n",
    "    # and the odia translations into the target into the target list.\n",
    "    inputs = [text for text in odia[\"src\"]]\n",
    "    target = [text for text in odia[\"tgt\"]]\n",
    "\n",
    "    model_inputs = {\n",
    "        \"id\": odia[\"idx\"],\n",
    "        \"translation\": target,  # assign 'translation' directly to 'target'\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": [],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Tokenize all inputs and targets at once, which is more efficient\n",
    "        tokenized_samples = tokenizer(inputs, max_length=128, truncation=True)\n",
    "        labels = tokenizer(target, max_length=128, truncation=True)\n",
    "\n",
    "        # Assign the tokenized samples directly to 'input_ids', 'attention_mask', and 'labels'\n",
    "        model_inputs[\"input_ids\"] = tokenized_samples[\"input_ids\"]\n",
    "        model_inputs[\"attention_mask\"] = tokenized_samples[\"attention_mask\"]\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "5d9f0398833e4db8b77606e590160717",
      "e8b2efc3a5e041388b4d90c8debb38f9",
      "300ca3c6bc9d4da698c8498535e2e2cc",
      "ee993c7561264c5ba04e79099725634a",
      "00a819bf8d214c638a704fc6e992497e",
      "7b453130961f49bea5d4e1e2ede42b81",
      "045c741c3cd94f8f9d09367fcf9a8361",
      "a11b4dda88734ad6a34660c30da15f6a",
      "29b80726611a4dafa3eccdaa8dc56e2f",
      "2765b57240344c9e82a66958397f2362",
      "bf6ec983cef54f47a4f6922e9c3ac42b",
      "f3f4bda5f1514ad098d837dbb68166bb",
      "dc965dae7a2f4755ba69236b5905f2cd",
      "a99d7c61ab05481eab42b04eadacb9e1",
      "08da1bad4f334ef896e6c74629c4ee08",
      "b6ec4e42dda24cfb8074a7e6ba598186",
      "63e46b4765434c6cb398a33449ee401d",
      "d1dc8167dc274a96974015c28709054e",
      "3ab1b0bd72af48fea7c1e829fe793e66",
      "392fa683dccf44eea8b7ddd991e2324c",
      "ea592b5af09e44cd8648d9c406b40d96",
      "96f3c73b058d47c9826982127858e82a"
     ]
    },
    "id": "5CFuZC1R_gPN",
    "outputId": "9d982546-e460-49ba-d96d-7ac3f53a0b95"
   },
   "outputs": [],
   "source": [
    "# Applying the tokenization funtion on the dataset, with batch processing\n",
    "tokenized_odia = odia.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXjRVTyf_mxl",
    "outputId": "b3477b14-a91e-4fc9-d047-f310e3f0b00b"
   },
   "outputs": [],
   "source": [
    "# after tokenization , the dataset\n",
    "print(tokenized_odia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbIJkvX8tWEs"
   },
   "outputs": [],
   "source": [
    "# importing the data collators which creates the batches of inputs to be fed to the seq2seq model.\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, model=\"facebook/nllb-200-distilled-600M\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CUfLuQ8t6uJ"
   },
   "outputs": [],
   "source": [
    "# Importing sacrebleu to evaluate the blue score of the model.\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyBa2w6zt-Qf"
   },
   "outputs": [],
   "source": [
    "# funtion to calculate and process the blue score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def postprocess(pred, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds, decoded_labels = postprocess(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5AaDxtHuCu4"
   },
   "outputs": [],
   "source": [
    "# trying to create a trainer using Pytorch\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3yKKd2ouFFD"
   },
   "outputs": [],
   "source": [
    "# tuning the hyperparameters of Seq2SeqTrainingArguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"fine_tune_model\",\n",
    "    report_to=\"wandb\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=6,\n",
    "    per_device_eval_batch_size=6,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    eval_accumulation_steps=50,\n",
    "    gradient_accumulation_steps=4,\n",
    "    predict_with_generate=True,\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "# passing the arguments to trainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_odia[\"train\"],\n",
    "    eval_dataset=tokenized_odia[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "id": "JOvHKilAuOKc",
    "outputId": "190aa771-4150-4f7d-987f-ae0876a0d372"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4X2uc81OydnF"
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"/home/jupyter/notebooks/notebook/fine-tune/\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
